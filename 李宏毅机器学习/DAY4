Gradient Descent梯度下降
Tip1: 调整学习速率
      a.自适应学习速率：在刚开始时候，可以使用较大的学习率，更新几次参数后就可以采用较小的学习速率
      b.Adagrad算法：每个参数的学习率都把它除上之前微分的均方根
                     单参数时候，在 Adagrad 中，当梯度越大的时候，步伐应该越大，但下面分母又导致当梯度越大的时候，步伐会越小。
Tip2: 随机梯度下降法
      常规梯度下降法走一步要处理到所有二十个例子，但随机算法此时已经走了二十步（每处理一步就更新）
Tip3: 特征缩放
      两个输入的分布的范围很不一样，建议把他们的范围缩放，使得不同输入的范围是一样的。
      使用的缩放方法：数据标准化

梯度下降的限制：容易陷入局部极值 还有可能卡在不是极值，但微分值是0的地方 还有可能实际中只是当微分值小于某一个数值就停下来了，但这里只是比较平缓，并不是极值点
